# Files
root_dir: "/home/nmiolane/code/proteo"
ray_tmp_dir: "/home/nmiolane/code/proteo/outputs/ray"  # "/tmp"
# Paths below are relative to root_dir
wandb_api_key_path: "wandb_api_key.txt"
data_dir: "data"
output_dir: "outputs"
checkpoint_dir: "outputs/checkpoints"
ray_results_dir: "outputs/ray_results"

# Dataset
dataset_name: "ftd"  # 352 in train, 89 in val
raw_file_name: "ALLFTD_dataset_for_nina_louisa.csv"

# WandB
project: "proteo"
wandb_offline: false
# Controls the frequency of logging within training, 
# by specifying how many training steps should occur between each logging event.
log_every_n_steps: 5

# Dataset
adj_thresh: 0.8  #0.08 # Note: n_edges(0.8) = 11367; n_edges(0.9)= 7467; n_edges(0.95)= 7303
wgcna_power: 6 #replaced this with softThreshold
wgcna_minModuleSize: 10
wgcna_mergeCutHeight: 0.25
task_type: mse_regression

# Model
model: gat

gat-v4:
  heads: [4, 3, 4]
  hidden_channels: [8, 16, 12]
  use_layer_norm: true
  which_layer: ['layer1', 'layer2', 'layer3']
  fc_dim: [64, 48, 32, 32]
  dropout: 0.05
  fc_act: elu
  num_nodes: 7289 # TODO: This should not be hardcoded
  # Training gat-v4
  lr: 0.1 
  weight_decay: 0.0005
  l1_lambda: 0.005
  optimizer: Adam
  lr_scheduler: LambdaLR

gat:
  heads: 4
  hidden_channels: 256  # needs to be divisible by heads
  num_layers: 2
  dropout: 0.2
  act: tanh
  v2: true
  # Training gat
  lr: 0.001
  weight_decay: 0.0005
  l1_lambda: 0.0005
  optimizer: Adam
  lr_scheduler: LambdaLR

gcn:
  hidden_channels: 32
  num_layers: 3
  dropout: 0.2
  act: relu
  # Training gcn
  lr: 0.001
  weight_decay: 0.0005
  l1_lambda: 0.0005
  optimizer: Adam
  lr_scheduler: LambdaLR

# Training
batch_size: 16 #for one off training
epochs: 100
num_workers: 16
sync_batchnorm: false
precision: "32-true"
accumulate_grad_batches: 1

# GPUs
device: [0] #, 1] #, 2, 3, 4, 5, 6, 7] 
trainer_accelerator: "gpu"
seed: 43

# Miscellaneous
use_progress_bar: true
nodes_count: 1
pin_memory: true

# Hyperparameter search
# Nb of trials = len(model_grid_search) * num_samples
num_samples: 1
grace_period: 4  # Each training is run at least 4 epochs, until epoch #3.
reduction_factor: 8
num_to_keep: 3  # Nb of checkpoints to keep. High value -> checkpoints overwritten less often.
checkpoint_interval: 5  # Nb of iterations between checkpoints. If 0: no checkpointing.
cpu_per_worker: 64
gpu_per_worker: 1  # FIXME: Only 1 gpu shows GPU Util > 0%
lr_min: 0.0001
lr_max: 0.1
model_grid_search: ['gat']  #, 'gat', 'gcn']
# Choices
batch_size_choice: [32] #, 4, 16, 32]  # note: 352 train and 89 val --> 3 val batch for bs = 32
scheduler_choice: ['LambdaLR'] #, 'ReduceLROnPlateau', 'ExponentialLR', 'StepLR', 'CosineAnnealingLR']
num_layers_choice: [2] #, 3, 4]  # only for GAT and GCN
dropout_choice: [0.2] #, 0.3, 0.8]
# GAT
gat_hidden_channels: [256] #8, 32, 128, 256]
gat_heads: [4]  #1, 2, 4]
# GAT-v4
gat_v4_hidden_channels: [[8, 16, 12], [32, 64, 64], [64, 128, 128]]
gat_v4_heads: [[2, 3, 4], [2, 2, 3], [4, 4, 4]]
# GCN
gcn_hidden_channels: [8, 32, 128]