# Description of parameters: 
# https://github.com/twitter-research/cwn/blob/main/exp/parser.py

dataset_name: "ftd"
adj_thresh: 0.08
wgcna_power: 6
wgcna_minModuleSize: 10
wgcna_mergeCutHeight: 0.25
task: 'survival'  
task_type: regression # Placeholder
minimize: true #Placeholder
#TODO: add root_dir to the config file, folders of data for ftd and mlagnn

# Model Configuration
model: gat-v4

gat-v4:
  lambda_cox: 1.0
  lambda_reg: 0.0003
  lambda_nll: 1.0
  heads: [4, 3, 4]
  hidden_channels: [8, 16, 12]
  fc_dim: [64, 48, 32]
  which_layer: ['layer1', 'layer2', 'layer3']
  cnv_dim: 0
  omic_dim: 32
  act_type: 'none'
  lr: 0.0001  #TODO: start with a higher learning rate, to espace the bad initialization
  final_lr: 0.1
  weight_decay: 0.0005
  fc_dropout: 0.2  # was dropout
  alpha: 0.2
  patience: 0.005
  # Optimizer Configuration
  lr_scheduler: LambdaLR
  lr_scheduler_params: None
  num_nodes: 7289 #TO DO: This should not be hardcoded
  layer_norm: true
  

higher-gat:
  nonlinearity: relu
  num_layers: 4
  readout: sum
  readout_dims: (0, 1, 2)
  max_ring_size: 18
  minimize: true
  hidden_channels: 64
  heads: 1
  # Optimizer Configuration
  lr: 0.001
  graph_norm: bn
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_params:
    mode: #TO DO
    lr_scheduler_patience: 20
    lr_scheduler_decay_steps: 50
    lr_scheduler_decay_rate: 0.5
    lr_scheduler_min: 0.00001
    verbose: true

gat:
  hidden_channels: 64
  heads: 1
  minimize: true #Placeholder
  # Optimizer Configuration
  lr: 0.001
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_params:
    mode: #TO DO
    lr_scheduler_patience: 20
    lr_scheduler_decay_steps: 50
    lr_scheduler_decay_rate: 0.5
    lr_scheduler_min: 0.00001
    verbose: true

early_stop: true




# Training Configuration
epochs: 50
batch_size: 12
in_drop_rate: 0.0
drop_rate: 0.0
drop_position: lin2
emb_dim: 128
preproc_jobs: 32
jump_mode: None
num_workers: 16
iso_eps: 0.01
sync_batchnorm: false
precision: "32-true"
accumulate_grad_batches: 1

# Evaluation Configuration
train_eval_period: 20
eval_metric: mae

# WandB Configuration
project_name: "proteo"
wandb_api_key_path: "wandb_api_key.txt"
wandb_offline: false
log_every_n_steps: 1

# GPU Configuration
device: [0, 1, 2, 3, 4, 5, 6, 7]
trainer_accelerator: "gpu"
seed: 43
start_seed: 0
stop_seed: 9

# Miscellaneous Configuration
use_progress_bar: true
checkpoint_dir: "checkpoints/"
checkpoint_name_pattern: "ckpt"
nodes_count: 1
pin_memory: true
result_folder: os.path.join(ROOT_DIR, 'exp', 'results')
untrained: false
paraid: 0

